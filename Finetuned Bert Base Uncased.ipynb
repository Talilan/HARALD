{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4195,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:55:53.535959Z",
     "iopub.status.busy": "2022-08-04T10:55:53.535165Z",
     "iopub.status.idle": "2022-08-04T10:56:02.402549Z",
     "shell.execute_reply": "2022-08-04T10:56:02.401676Z",
     "shell.execute_reply.started": "2022-08-04T10:55:53.535916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Dani1.OFER3090PC\\\\Documents\\\\Tal Folder\\\\thesis\\\\updated files\\\\most updated files'"
      ]
     },
     "execution_count": 4195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, auc,classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, Adafactor\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import time\n",
    "#from babel.dates import format_time\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "device = torch.device(\"cuda\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4196,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:07.681439Z",
     "iopub.status.busy": "2022-08-04T10:56:07.680748Z",
     "iopub.status.idle": "2022-08-04T10:56:07.825276Z",
     "shell.execute_reply": "2022-08-04T10:56:07.824296Z",
     "shell.execute_reply.started": "2022-08-04T10:56:07.681398Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dani1.OFER3090PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4197,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:10.953452Z",
     "iopub.status.busy": "2022-08-04T10:56:10.952778Z",
     "iopub.status.idle": "2022-08-04T10:56:11.052141Z",
     "shell.execute_reply": "2022-08-04T10:56:11.051080Z",
     "shell.execute_reply.started": "2022-08-04T10:56:10.953415Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dani1.OFER3090PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4198,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:12.649948Z",
     "iopub.status.busy": "2022-08-04T10:56:12.649553Z",
     "iopub.status.idle": "2022-08-04T10:56:12.657081Z",
     "shell.execute_reply": "2022-08-04T10:56:12.656394Z",
     "shell.execute_reply.started": "2022-08-04T10:56:12.649913Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Dani1.OFER3090PC\\\\Documents\\\\Tal Folder\\\\thesis\\\\updated files\\\\most updated files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4199,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:14.158173Z",
     "iopub.status.busy": "2022-08-04T10:56:14.157559Z",
     "iopub.status.idle": "2022-08-04T10:56:14.164856Z",
     "shell.execute_reply": "2022-08-04T10:56:14.163976Z",
     "shell.execute_reply.started": "2022-08-04T10:56:14.158124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4200,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:16.198267Z",
     "iopub.status.busy": "2022-08-04T10:56:16.197221Z",
     "iopub.status.idle": "2022-08-04T10:56:16.206523Z",
     "shell.execute_reply": "2022-08-04T10:56:16.205644Z",
     "shell.execute_reply.started": "2022-08-04T10:56:16.198226Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "\n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        output = self.bert(sent_id, attention_mask=mask)\n",
    "       #print(cls_hs)\n",
    "        #output = output[\"pooler_output\"]\n",
    "        output = output[1]\n",
    "                \n",
    "        x = self.fc1(output)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4201,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:19.181383Z",
     "iopub.status.busy": "2022-08-04T10:56:19.180931Z",
     "iopub.status.idle": "2022-08-04T10:56:19.192680Z",
     "shell.execute_reply": "2022-08-04T10:56:19.191677Z",
     "shell.execute_reply.started": "2022-08-04T10:56:19.181328Z"
    }
   },
   "outputs": [],
   "source": [
    "#function to train the model\n",
    "def train(model,train_dataloader, cross_entropy, optimizer ):\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4202,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:22.529795Z",
     "iopub.status.busy": "2022-08-04T10:56:22.529395Z",
     "iopub.status.idle": "2022-08-04T10:56:22.539304Z",
     "shell.execute_reply": "2022-08-04T10:56:22.538099Z",
     "shell.execute_reply.started": "2022-08-04T10:56:22.529750Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader, cross_entropy):\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "  \n",
    "  t0 = time.time()\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      #elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "        loss = cross_entropy(preds,labels)\n",
    "\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "      #torch.cuda.empty_cache()  \n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4203,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:26.050588Z",
     "iopub.status.busy": "2022-08-04T10:56:26.049952Z",
     "iopub.status.idle": "2022-08-04T10:56:26.442228Z",
     "shell.execute_reply": "2022-08-04T10:56:26.441174Z",
     "shell.execute_reply.started": "2022-08-04T10:56:26.050549Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets_names = [\n",
    "                   'Kaggle',\n",
    "                   'Founta',\n",
    "                    'Razavi',\n",
    "                   'Waseem',\n",
    "                   'Kumar',\n",
    "                   #'RoastMe',\n",
    "                    'Offensive_Reddit_new'\n",
    "                  ]\n",
    "datasets_dict = {}\n",
    "for datasets_name in datasets_names:\n",
    "     path = datasets_name+'_Data_balanced.csv'\n",
    "     dataset = pd.read_csv(path)\n",
    "     datasets_dict[datasets_name] = dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4204,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T10:19:14.739372Z",
     "iopub.status.busy": "2022-08-02T10:19:14.738868Z",
     "iopub.status.idle": "2022-08-02T10:19:14.745957Z",
     "shell.execute_reply": "2022-08-02T10:19:14.744737Z",
     "shell.execute_reply.started": "2022-08-02T10:19:14.739336Z"
    }
   },
   "outputs": [],
   "source": [
    "#datasets_dict['Waseem'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4205,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T10:19:14.747821Z",
     "iopub.status.busy": "2022-08-02T10:19:14.747290Z",
     "iopub.status.idle": "2022-08-02T10:19:14.800598Z",
     "shell.execute_reply": "2022-08-02T10:19:14.799792Z",
     "shell.execute_reply.started": "2022-08-02T10:19:14.747785Z"
    }
   },
   "outputs": [],
   "source": [
    "#datasets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4206,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:33.556255Z",
     "iopub.status.busy": "2022-08-04T10:56:33.555853Z",
     "iopub.status.idle": "2022-08-04T10:56:33.564226Z",
     "shell.execute_reply": "2022-08-04T10:56:33.563117Z",
     "shell.execute_reply.started": "2022-08-04T10:56:33.556222Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4207,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:35.743442Z",
     "iopub.status.busy": "2022-08-04T10:56:35.742872Z",
     "iopub.status.idle": "2022-08-04T10:56:35.752238Z",
     "shell.execute_reply": "2022-08-04T10:56:35.750808Z",
     "shell.execute_reply.started": "2022-08-04T10:56:35.743402Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_preproc(x):\n",
    "  x = x.lower()\n",
    "  x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "  x = x.encode('ascii', 'ignore').decode()\n",
    "  x = re.sub(r'https*\\S+', ' ', x)\n",
    "  x = re.sub(r'@\\S+', ' ', x)\n",
    "  x = re.sub(r'#', ' ', x)\n",
    "  x = re.sub(r'\\'\\w+', '', x)\n",
    "  x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "  x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "  x = re.sub(r'\\s{2,}', ' ', x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4208,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:37.896127Z",
     "iopub.status.busy": "2022-08-04T10:56:37.895295Z",
     "iopub.status.idle": "2022-08-04T10:56:37.902503Z",
     "shell.execute_reply": "2022-08-04T10:56:37.901251Z",
     "shell.execute_reply.started": "2022-08-04T10:56:37.896083Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_dataset(dataset):\n",
    "    df = pd.DataFrame()\n",
    "    df['clean_text'] = dataset.comment.apply(text_preproc)\n",
    "    dataset['clean_comment'] = df['clean_text']\n",
    "    col_list = list(dataset)\n",
    "    col_list[2], col_list[3] = col_list[3], col_list[2]\n",
    "    dataset = dataset[col_list]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4209,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:41.553794Z",
     "iopub.status.busy": "2022-08-04T10:56:41.553002Z",
     "iopub.status.idle": "2022-08-04T10:56:48.700405Z",
     "shell.execute_reply": "2022-08-04T10:56:48.698914Z",
     "shell.execute_reply.started": "2022-08-04T10:56:41.553744Z"
    }
   },
   "outputs": [],
   "source": [
    "for dataset_name in datasets_names: \n",
    "    datasets_dict[dataset_name] = clean_dataset(datasets_dict[dataset_name])\n",
    "#datasets_dict['Kaggle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4210,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T10:19:21.362958Z",
     "iopub.status.busy": "2022-08-02T10:19:21.362240Z",
     "iopub.status.idle": "2022-08-02T10:19:21.369442Z",
     "shell.execute_reply": "2022-08-02T10:19:21.368685Z",
     "shell.execute_reply.started": "2022-08-02T10:19:21.362903Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_dataset = pd.read_csv('train_dataset_1_Kaggle.csv')\n",
    "# train_dataset = clean_dataset(train_dataset)\n",
    "# len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4211,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T10:19:21.371716Z",
     "iopub.status.busy": "2022-08-02T10:19:21.371024Z",
     "iopub.status.idle": "2022-08-02T10:19:21.381828Z",
     "shell.execute_reply": "2022-08-02T10:19:21.381175Z",
     "shell.execute_reply.started": "2022-08-02T10:19:21.371680Z"
    }
   },
   "outputs": [],
   "source": [
    "#datasets_dict['Offensive_Reddit_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4212,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T10:19:21.385611Z",
     "iopub.status.busy": "2022-08-02T10:19:21.385009Z",
     "iopub.status.idle": "2022-08-02T10:19:21.390896Z",
     "shell.execute_reply": "2022-08-02T10:19:21.390257Z",
     "shell.execute_reply.started": "2022-08-02T10:19:21.385576Z"
    }
   },
   "outputs": [],
   "source": [
    "# col_list = list(datasets_dict['Waseem'])\n",
    "# col_list[2], col_list[3] = col_list[3], col_list[2]\n",
    "# datasets_dict['Waseem'] = datasets_dict['Waseem'][col_list]\n",
    "# datasets_dict['Waseem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4213,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T10:19:21.393552Z",
     "iopub.status.busy": "2022-08-02T10:19:21.392767Z",
     "iopub.status.idle": "2022-08-02T10:19:21.401121Z",
     "shell.execute_reply": "2022-08-02T10:19:21.400069Z",
     "shell.execute_reply.started": "2022-08-02T10:19:21.393515Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.to_csv('clean_waseem.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4214,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:53.622104Z",
     "iopub.status.busy": "2022-08-04T10:56:53.621513Z",
     "iopub.status.idle": "2022-08-04T10:56:53.649509Z",
     "shell.execute_reply": "2022-08-04T10:56:53.648399Z",
     "shell.execute_reply.started": "2022-08-04T10:56:53.622057Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_bert(train_text,train_labels,val_text,val_labels,test_text, test_labels):\n",
    "    bert = AutoModel.from_pretrained('bert-base-uncased')    \n",
    "    # Load the BERT tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')    \n",
    "    # sample data\n",
    "    text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
    "    # encode text\n",
    "    sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\n",
    "    print(sent_id)\n",
    "# tokenize and encode sequences in the training set\n",
    "    max_seq_len = 180\n",
    "    tokens_train = tokenizer.batch_encode_plus(\n",
    "        train_text.tolist(),\n",
    "        max_length = max_seq_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "        )    \n",
    "    \n",
    "    tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    "    )\n",
    "    \n",
    "    tokens_test= tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    "    )\n",
    "    # for train set\n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "    train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "    val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "    test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "    test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "    test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "    len(test_mask)\n",
    "\n",
    "\n",
    "#define a batch size\n",
    "    batch_size = 4\n",
    "\n",
    "# wrap tensors\n",
    "    train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "    val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "    val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
    "\n",
    "# freeze all the parameters\n",
    "    #print(bert.parameters().size())\n",
    "\n",
    "    #length = 0\n",
    "#     for param in bert.parameters():\n",
    "#         length += 1\n",
    "#         param.requires_grad = False\n",
    "#     print(\"parameters length: \")\n",
    "#     print(length)\n",
    "#     for name, param in bert.named_parameters(): \n",
    "#         if param.requires_grad == True:\n",
    "#             print(name)\n",
    "    for name, param in list(bert.named_parameters())[:101]: \n",
    "        #print('I will be frozen: {}'.format(name)) \n",
    "        param.requires_grad = False\n",
    "#     for name, param in bert.named_parameters():\n",
    "#         if param.requires_grad == True:\n",
    "#             print(name) \n",
    "    \n",
    "\n",
    "\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "    model = BERT_Arch(bert)\n",
    "# push the model to GPU\n",
    "    model = model.to(device)\n",
    "# define the optimizer\n",
    "    #optimizer = AdamW(model.parameters(), lr = 1e-4)\n",
    "    optimizer = Adafactor(model.parameters(),lr = 2e-5, relative_step =False)\n",
    "    \n",
    "#compute the class weights\n",
    "    #class_wts = compute_class_weight(dict(zip(np.unique(train_labels), train_labels)))\n",
    "    class_wts = compute_class_weight(class_weight = 'balanced', classes = np.unique(train_labels), y =train_labels)\n",
    "    #class_wts = dict(zip(np.unique(train_labels), class_wts))\n",
    "\n",
    "    print(class_wts)\n",
    "# convert class weights to tensor\n",
    "    weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "    weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "    cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "    epochs = 4\n",
    "\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "    train_losses=[]\n",
    "    valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "    torch.cuda.empty_cache()\n",
    "    for epoch in range(epochs):\n",
    "     \n",
    "        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "        train_loss, train_preds = train(model, train_dataloader, cross_entropy, optimizer )\n",
    "    \n",
    "    #evaluate model\n",
    "        valid_loss, _ = evaluate(model, val_dataloader,cross_entropy)\n",
    "    \n",
    "    #save the best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            #torch.save(model.state_dict(), '/kaggle/working/Bert_Models/Kaggle/1/saved_weights_Kaggle.pt')\n",
    "            #torch.save(model.state_dict(), '/kaggle/input/data-cross-domain/saved_weights_Kaggle.pt')\n",
    "            final_model = model\n",
    "    \n",
    "    # append training and validation loss\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "    \n",
    "        print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "        print(f'Validation Loss: {valid_loss:.3f}')\n",
    "        #torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #load weights of best model saved_model_Kaggle_1.pt\n",
    "    #path = '/kaggle/input/data-cross-domain/saved_weights_Kaggle.pt'\n",
    "    #output_model_dir = 'C:\\\\Users\\\\Dani1.OFER3090PC\\\\Documents\\\\Tal Folder\\\\thesis\\\\updated files\\\\Bert_Models\\\\Kaggle\\\\1'\n",
    "    #model.load_state_dict(torch.load(path))\n",
    "#     model_to_save = (\n",
    "#     model.module if hasattr(model, \"module\") else model\n",
    "#     )  # Take care of distributed/parallel training\n",
    "    #model_to_save.save_pretrained(output_model_dir)\n",
    "    #tokenizer.save_pretrained(output_model_dir)\n",
    "#     torch.save(model_to_save.state_dict(),output_model_dir)\n",
    "    test_mask\n",
    "    \n",
    "    #torch.cuda.empty_cache()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds_test = final_model(test_seq.to(device), test_mask.to(device))\n",
    "        preds_test = preds_test.detach().cpu().numpy()\n",
    "        #preds_train = preds_train.detach().cpu().numpy()\n",
    "        \n",
    "    # model's performance\n",
    "    preds_test = np.argmax(preds_test, axis = 1)\n",
    "    Report = classification_report(test_y, preds_test, output_dict=True )\n",
    "    print(Report)\n",
    "\n",
    "    # confusion matrix\n",
    "    print(pd.crosstab(test_y, preds_test))\n",
    "    confusion_matrix = pd.crosstab(test_y, preds_test)\n",
    "    \n",
    "    FP = []\n",
    "    FN = []\n",
    "    TN = []\n",
    "    TP = []\n",
    "    for i in range(len(test_y)):\n",
    "        if test_y[i] == 0 and  preds_test[i] ==1:\n",
    "            FP.append(test_text[i])\n",
    "        elif test_y[i] == 1 and  preds_test[i] ==0:\n",
    "            FN.append(test_text[i])\n",
    "        elif test_y[i] == 0 and  preds_test[i] ==0:\n",
    "            TN.append(test_text[i])\n",
    "        elif test_y[i] == 1 and  preds_test[i] ==1:\n",
    "            TP.append(test_text[i])         \n",
    "#     print(\"10 False Positives: \\n\")\n",
    "#     print(FP[0:11])\n",
    "#     print(\"10 False Negatives: \\n\")\n",
    "#     print(FN[0:11]) \n",
    "#     print(\"10 True Negatives: \\n\")\n",
    "#     print(TN[0:11])\n",
    "#     print(\"10 True Positives: \\n\")\n",
    "#     print(TP[0:11]) \n",
    "    return Report,confusion_matrix, FP, FN, TN, TP, preds_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4215,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:56:59.516226Z",
     "iopub.status.busy": "2022-08-04T10:56:59.515833Z",
     "iopub.status.idle": "2022-08-04T10:56:59.521319Z",
     "shell.execute_reply": "2022-08-04T10:56:59.520398Z",
     "shell.execute_reply.started": "2022-08-04T10:56:59.516193Z"
    }
   },
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    " \n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    "     \n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4216,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:57:06.162131Z",
     "iopub.status.busy": "2022-08-04T10:57:06.161579Z",
     "iopub.status.idle": "2022-08-04T10:57:06.173481Z",
     "shell.execute_reply": "2022-08-04T10:57:06.172552Z",
     "shell.execute_reply.started": "2022-08-04T10:57:06.162082Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_balanced_datasets(max_size,dataset_label_0,dataset_label_1):\n",
    "    #if len(dataset_label_0) + len(dataset_label_1) >  2*max_size:\n",
    "        minority_len = min(len(dataset_label_0),len(dataset_label_1))\n",
    "        if  minority_len > max_size:\n",
    "            balanced_0 = dataset_label_0.sample(max_size)\n",
    "            balanced_1 = dataset_label_1.sample(max_size)\n",
    "        else:\n",
    "            balanced_0 = dataset_label_0.sample(minority_len)\n",
    "            balanced_1 = dataset_label_1.sample(minority_len)\n",
    "        balanced_dataset = pd.concat([balanced_0,balanced_1],axis=0,ignore_index=True)\n",
    "        balanced_dataset = balanced_dataset.sample(frac=1).reset_index(drop=True)\n",
    "        balanced_dataset['id'] = [i for i in range(1,len(balanced_dataset)+1)]\n",
    "        #balanced_dataset = shuffle(balanced_dataset)\n",
    "        return balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4217,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:57:08.743364Z",
     "iopub.status.busy": "2022-08-04T10:57:08.742922Z",
     "iopub.status.idle": "2022-08-04T10:57:08.750282Z",
     "shell.execute_reply": "2022-08-04T10:57:08.749442Z",
     "shell.execute_reply.started": "2022-08-04T10:57:08.743313Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_imbalanced_datasets(majority_rate,dataset_label_0,dataset_label_1):\n",
    "    #if len(dataset_label_0) + len(dataset_label_1) >  2*max_size:\n",
    "        majority_len = len(dataset_label_0)\n",
    "        minority_len  = int(np.ceil(majority_len/majority_rate)) - majority_len\n",
    "        imbalanced_0 = dataset_label_0.sample(majority_len)\n",
    "        imbalanced_1 = dataset_label_1.sample(minority_len)\n",
    "        imbalanced_dataset = pd.concat([imbalanced_0,imbalanced_1],axis=0,ignore_index=True)\n",
    "        imbalanced_dataset = imbalanced_dataset.sample(frac=1).reset_index(drop=True)\n",
    "        imbalanced_dataset['id'] = [i for i in range(1,len(imbalanced_dataset)+1)]\n",
    "        #balanced_dataset = shuffle(balanced_dataset)\n",
    "        return imbalanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4218,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T10:59:58.703387Z",
     "iopub.status.busy": "2022-08-04T10:59:58.702919Z",
     "iopub.status.idle": "2022-08-04T10:59:58.713438Z",
     "shell.execute_reply": "2022-08-04T10:59:58.712094Z",
     "shell.execute_reply.started": "2022-08-04T10:59:58.703325Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_imbalnced_from_balanced(n,majority_rate,train_dataset, train_dataset_name):\n",
    "    for i in range(n):\n",
    "        train_dataset = pd.read_csv('train_train_dataset_'+ str(i+1)+'_'+train_dataset_name+'.csv')\n",
    "        train_dataset = clean_dataset(train_dataset) \n",
    "        train_dataset = create_imbalanced_datasets(majority_rate,train_dataset[train_dataset['Label'] == 0],train_dataset[train_dataset['Label'] == 1])\n",
    "        validation_dataset = pd.read_csv('validation_train_dataset_'+ str(i+1)+'_'+train_dataset_name+'.csv')\n",
    "        validation_dataset = clean_dataset(validation_dataset) \n",
    "        validation_dataset = create_imbalanced_datasets(majority_rate,validation_dataset[validation_dataset['Label'] == 0],validation_dataset[validation_dataset['Label'] == 1])\n",
    "        train_dataset.to_csv('/kaggle/working/train_train_dataset_'+str(i+1)+'_'+train_dataset_name + '_imbalanced.csv',index=False)\n",
    "        validation_dataset.to_csv('/kaggle/working/validation_train_dataset_'+str(i+1)+'_'+train_dataset_name + '_imbalanced.csv',index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4219,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T11:00:18.738226Z",
     "iopub.status.busy": "2022-08-04T11:00:18.737516Z",
     "iopub.status.idle": "2022-08-04T11:00:19.933925Z",
     "shell.execute_reply": "2022-08-04T11:00:19.932950Z",
     "shell.execute_reply.started": "2022-08-04T11:00:18.738179Z"
    }
   },
   "outputs": [],
   "source": [
    "#create_imbalnced_from_balanced(4,0.7,datasets_dict['Razavi'],'Razavi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4220,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T10:19:21.545163Z",
     "iopub.status.busy": "2022-08-02T10:19:21.544723Z",
     "iopub.status.idle": "2022-08-02T10:19:21.565730Z",
     "shell.execute_reply": "2022-08-02T10:19:21.565049Z",
     "shell.execute_reply.started": "2022-08-02T10:19:21.545130Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_n_times_on_other_dataset(n,train_dataset_name_1,train_dataset_name_2,test_dataset,val_rate,train_dataset_number):\n",
    "    f1_1 = 0\n",
    "    f1_weighted = 0\n",
    "    FP, FN, TN, TP = [],[],[],[]\n",
    "    preds = []\n",
    "    best_f1 = 0\n",
    "    #dataset_number = 1\n",
    "    #n=7\n",
    "    for i in range(n):\n",
    "        #train_dataset = pd.read_csv('train_dataset_'+ str(train_dataset_number)+'_'+train_dataset_name_1+'_'+train_dataset_name_2+'.csv')\n",
    "        train_dataset = pd.read_csv('train_dataset_'+ str(train_dataset_number)+'_'+train_dataset_name_1+'_RM.csv')\n",
    "        train_dataset = clean_dataset(train_dataset)\n",
    "#         validation_dataset = pd.read_csv('validation_dataset_'+ str(dataset_number)+'_'+train_dataset_name+'_RM.csv')\n",
    "#         validation_dataset = clean_dataset(validation_dataset)\n",
    "        #train_dataset = create_balanced_datasets(1500,train_dataset[train_dataset['Label']==0],train_dataset[train_dataset['Label']==1])\n",
    "        #test_dataset = create_balanced_datasets(1000,test_dataset[test_dataset['Label']==0],test_dataset[test_dataset['Label']==1])\n",
    "        train_text, val_text, train_labels, val_labels = train_test_split(train_dataset['clean_comment'],\n",
    "                                                         train_dataset['Label'],test_size=val_rate, stratify = train_dataset['Label'])\n",
    "#         train_text, val_text, train_labels, val_labels = train_test_split(train_dataset['comment'], train_dataset['Label'], \n",
    "#                                                                     #random_state=2018,\n",
    "#                                                                     test_size= 1-train_rate,\n",
    "#                                                                     stratify=train_dataset['Label'])\n",
    "#         _, val_text, _, val_labels = train_test_split(rest_text, rest_labels, \n",
    "#                                                                     random_state=2018,\n",
    "#                                                                     test_size=val_rate,\n",
    "#                                                                     stratify=rest_labels)\n",
    "#         _, test_text, _, test_labels  = train_test_split(test_dataset['comment'], test_dataset['Label'], \n",
    "#                                                                     random_state=2018,\n",
    "#                                                                     test_size=test_rate,\n",
    "#                                                                     stratify=test_dataset['Label']\n",
    "#                                                                     )\n",
    "        print(\"train length: \" + str(len(train_text)))\n",
    "        print(np.sum(val_labels == 0))\n",
    "        print(\"train length 0 label: \" + str(np.sum(train_labels == 0)))\n",
    "        print(\"train length 1 label: \" + str(np.sum(train_labels == 1)))\n",
    "        print(\"val length: \" + str(len(val_text)))\n",
    "        print(\"val length 0 label: \" + str(np.sum(val_labels == 0)))\n",
    "        print(\"val length 1 label: \" + str(np.sum(val_labels == 1)))\n",
    "        print(\"test length: \" + str(len(test_dataset['comment'])))\n",
    "        print(\"test length 0 label: \" + str(len(test_dataset['Label'][test_dataset['Label']==0])))\n",
    "        print(\"test length 1 label: \" + str(len(test_dataset['Label'][test_dataset['Label']==1])))\n",
    "        \n",
    "        #torch.cuda.empty_cache()\n",
    "        report,confusion_matrix, FP_new, FN_new, TN_new, TP_new, preds_test  = run_bert(train_text,train_labels,val_text,val_labels,test_dataset['clean_comment'], test_dataset['Label'])\n",
    "        f1_1 += report['1']['f1-score']\n",
    "        f1_weighted += report['weighted avg']['f1-score']\n",
    "        if report['weighted avg']['f1-score'] > best_f1:\n",
    "            best_f1 = report['weighted avg']['f1-score']\n",
    "            best_report = report\n",
    "            best_conf_mat = confusion_matrix\n",
    "        FP = FP + FP_new\n",
    "        FN = FN + FN_new\n",
    "        TN = TN + TN_new\n",
    "        TP = TP + TP_new\n",
    "        #preds = preds + preds_test\n",
    "    f1_1 = f1_1/n\n",
    "    f1_weighted = f1_weighted/n\n",
    "    \n",
    "    print(f1_1)\n",
    "    #test_dictionary['f1_1'] = f1_1\n",
    "    print(f1_weighted)\n",
    "    FP = unique(FP)\n",
    "    FN = unique(FN)\n",
    "    TN = unique(TN)\n",
    "    TP = unique(TP)\n",
    "    #test_dictionary['f1_weighted'] = f1_weighted\n",
    "    \n",
    "    return best_report,best_conf_mat, f1_1, f1_weighted, FP, FN, TN, TP, preds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4221,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T10:19:21.567327Z",
     "iopub.status.busy": "2022-08-02T10:19:21.566737Z",
     "iopub.status.idle": "2022-08-02T10:19:21.597746Z",
     "shell.execute_reply": "2022-08-02T10:19:21.596975Z",
     "shell.execute_reply.started": "2022-08-02T10:19:21.567294Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_for_train_dataset(train_dataset_name_1,train_dataset_name_2,datasets_dict,n_iter,val_rate,train_dataset_number, test_dataset_number):\n",
    "    test_dict = {}\n",
    "    #train_dataset = datasets_dict[train_dataset_name]\n",
    "\n",
    "    for dataset_name in datasets_dict:\n",
    "        #if dataset_name == train_dataset_name_1 or dataset_name == train_dataset_name_2:\n",
    "        if dataset_name == train_dataset_name_1:\n",
    "        #if dataset_name != 'Founta':\n",
    "            continue\n",
    "        else:\n",
    "            print(\"train on \" + train_dataset_name_1 + \" dataset and \" + train_dataset_name_2 + \":\\n\")  \n",
    "            print(\"test on: \" + dataset_name)\n",
    "            test_dict[dataset_name] = {}\n",
    "            test_dict[dataset_name]['dataset'] = datasets_dict[dataset_name]\n",
    "            #path = 'train_dataset'+'_'+ str(test_dataset_number)+'_'+ dataset_name +'.csv'\n",
    "            path = 'train_dataset'+'_'+ str(test_dataset_number)+'_'+ dataset_name +'.csv'\n",
    "            test_dataset = pd.read_csv(path)\n",
    "            #test_dataset = create_balanced_datasets(750,test_dataset[test_dataset['Label']==0],test_dataset[test_dataset['Label']==1])\n",
    "            test_dataset = clean_dataset(test_dataset)\n",
    "            test_dict[dataset_name]['best_report'],test_dict[dataset_name]['best_confusion_matrix'] ,test_dict[dataset_name]['f1_1'],test_dict[dataset_name]['f1_weighted'], test_dict[dataset_name]['FP'],test_dict[dataset_name]['FN'], test_dict[dataset_name]['TN'],test_dict[dataset_name]['TP'], test_dataset['predictions'] = test_n_times_on_other_dataset(n_iter,\n",
    "                                                                                                               train_dataset_name_1,\n",
    "                                                                                                               train_dataset_name_2,\n",
    "                                                                                                               test_dataset,\n",
    "                                                                                                               #,train_rate\n",
    "                                                                                                               val_rate,\n",
    "                                                                                                               train_dataset_number\n",
    "                                                                                                               #datasets_dict[dataset_name]['test_rate']\n",
    "                                                                                                                  )\n",
    "\n",
    "        print(\"test on: \"+dataset_name+\" Average f1 score for Label 1 is:\"+str(test_dict[dataset_name]['f1_1'])+'\\n')\n",
    "        print(\"test on: \" +dataset_name+\" Average f1 weighted score is:\"+str(test_dict[dataset_name]['f1_weighted'])+'\\n')\n",
    "        print(\"5 False Positives: {}\".format(test_dict[dataset_name]['FP'][0:6]))\n",
    "        print(\"5 False Negatives: {}\".format(test_dict[dataset_name]['FN'][0:6]))\n",
    "        print(\"5 True Positives: {}\".format(test_dict[dataset_name]['TP'][0:6]))\n",
    "        print(\"5 True Negatives: {}\".format(test_dict[dataset_name]['TN'][0:6]))\n",
    "        test_dataset.to_csv('test with predictions ' + dataset_name + ' trained by ' + train_dataset_name_1 + '_RM.csv', index=False)\n",
    "        with open( 'dict ' + train_dataset_name_1+ '_'+train_dataset_name_2 + '_test_on_' + dataset_name + '_' + str(train_dataset_number)+'_RM.pickle', 'wb') as handle:\n",
    "            pickle.dump(test_dict[dataset_name], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    \n",
    "#     with open( 'dict' + train_dataset_name+'_RM_imbalanced.pickle', 'wb') as handle:\n",
    "#         pickle.dump(test_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4222,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T10:19:21.600076Z",
     "iopub.status.busy": "2022-08-02T10:19:21.599136Z",
     "iopub.status.idle": "2022-08-02T10:21:54.860279Z",
     "shell.execute_reply": "2022-08-02T10:21:54.859396Z",
     "shell.execute_reply.started": "2022-08-02T10:19:21.600042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on Kumar dataset and :\n",
      "\n",
      "test on: Razavi\n",
      "train length: 2688\n",
      "337\n",
      "train length 0 label: 1350\n",
      "train length 1 label: 1338\n",
      "val length: 672\n",
      "val length 0 label: 337\n",
      "val length 1 label: 335\n",
      "test length: 771\n",
      "test length 0 label: 386\n",
      "test length 1 label: 385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dani1.OFER3090PC\\.conda\\envs\\rm-project\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99555556 1.0044843 ]\n",
      "\n",
      " Epoch 1 / 4\n",
      "  Batch    50  of    672.\n",
      "  Batch   100  of    672.\n",
      "  Batch   150  of    672.\n",
      "  Batch   200  of    672.\n",
      "  Batch   250  of    672.\n",
      "  Batch   300  of    672.\n",
      "  Batch   350  of    672.\n",
      "  Batch   400  of    672.\n",
      "  Batch   450  of    672.\n",
      "  Batch   500  of    672.\n",
      "  Batch   550  of    672.\n",
      "  Batch   600  of    672.\n",
      "  Batch   650  of    672.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    168.\n",
      "  Batch   100  of    168.\n",
      "  Batch   150  of    168.\n",
      "\n",
      "Training Loss: 0.688\n",
      "Validation Loss: 0.681\n",
      "\n",
      " Epoch 2 / 4\n",
      "  Batch    50  of    672.\n",
      "  Batch   100  of    672.\n",
      "  Batch   150  of    672.\n",
      "  Batch   200  of    672.\n",
      "  Batch   250  of    672.\n",
      "  Batch   300  of    672.\n",
      "  Batch   350  of    672.\n",
      "  Batch   400  of    672.\n",
      "  Batch   450  of    672.\n",
      "  Batch   500  of    672.\n",
      "  Batch   550  of    672.\n",
      "  Batch   600  of    672.\n",
      "  Batch   650  of    672.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    168.\n",
      "  Batch   100  of    168.\n",
      "  Batch   150  of    168.\n",
      "\n",
      "Training Loss: 0.664\n",
      "Validation Loss: 0.654\n",
      "\n",
      " Epoch 3 / 4\n",
      "  Batch    50  of    672.\n",
      "  Batch   100  of    672.\n",
      "  Batch   150  of    672.\n",
      "  Batch   200  of    672.\n",
      "  Batch   250  of    672.\n",
      "  Batch   300  of    672.\n",
      "  Batch   350  of    672.\n",
      "  Batch   400  of    672.\n",
      "  Batch   450  of    672.\n",
      "  Batch   500  of    672.\n",
      "  Batch   550  of    672.\n",
      "  Batch   600  of    672.\n",
      "  Batch   650  of    672.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    168.\n",
      "  Batch   100  of    168.\n",
      "  Batch   150  of    168.\n",
      "\n",
      "Training Loss: 0.627\n",
      "Validation Loss: 0.611\n",
      "\n",
      " Epoch 4 / 4\n",
      "  Batch    50  of    672.\n",
      "  Batch   100  of    672.\n",
      "  Batch   150  of    672.\n",
      "  Batch   200  of    672.\n",
      "  Batch   250  of    672.\n",
      "  Batch   300  of    672.\n",
      "  Batch   350  of    672.\n",
      "  Batch   400  of    672.\n",
      "  Batch   450  of    672.\n",
      "  Batch   500  of    672.\n",
      "  Batch   550  of    672.\n",
      "  Batch   600  of    672.\n",
      "  Batch   650  of    672.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    168.\n",
      "  Batch   100  of    168.\n",
      "  Batch   150  of    168.\n",
      "\n",
      "Training Loss: 0.586\n",
      "Validation Loss: 0.573\n",
      "{'0': {'precision': 0.5752508361204013, 'recall': 0.8911917098445595, 'f1-score': 0.6991869918699186, 'support': 386}, '1': {'precision': 0.7572254335260116, 'recall': 0.34025974025974026, 'f1-score': 0.4695340501792115, 'support': 385}, 'accuracy': 0.6160830090791181, 'macro avg': {'precision': 0.6662381348232065, 'recall': 0.61572572505215, 'f1-score': 0.584360521024565, 'support': 771}, 'weighted avg': {'precision': 0.666120122762632, 'recall': 0.6160830090791181, 'f1-score': 0.5845094528933659, 'support': 771}}\n",
      "col_0    0    1\n",
      "row_0          \n",
      "0      344   42\n",
      "1      254  131\n",
      "0.4695340501792115\n",
      "0.5845094528933659\n",
      "test on: Razavi Average f1 score for Label 1 is:0.4695340501792115\n",
      "\n",
      "test on: Razavi Average f1 weighted score is:0.5845094528933659\n",
      "\n",
      "5 False Positives: [' like tell us decision request ', ' think want microsoft joke ', ' us decision stop writing ', ' mon oct gmt beable van polasm fri oct lt wilkes pokemon tampons thoughts opinions suggestions already japan indeed do hello kitty pads too convenience store chyk saw plastic package picture hello kitty it said kitty reached pick up slapped hand shouted no later went back store without looked packet determine little mouse pillow pads women like use she slapped hand whats mouse pillow pad women like use obviously must get one since women like so government association men violence rest us tolstoy ', ' us name person becuase nobody else knows him ', ' who tad jesse oh godtad tad debbie yield princess ']\n",
      "5 False Negatives: [' uses advanced technology radio frequency scanning detection logs frequencies tuned passing cars detecting emissions local oscillators car radios measurement devices called electronic measurement units emus placed along busy roadways capable measuring thousands radios passing cars every day multiple units placed different geographical areas market combination yield daily sample sizes hundreds thousands providing unprecedented accuracy resolution obviously big brother listening sheesh now finding yet another way focus marketing people listen commercial radio always try stay left end dial commercial free zone put tape happy motoring randy barrow yeah saw randy electronics professional bit lunch really this anyone shed light detail local oscillator rf reaching street corner moving vehicles dumbfounded or dumb buy ', ' doctype html public dtd html transitional en html surprise that nbsp figured headless horseman is pamber blockquote type citeof sleepy hollow myself oh fucking well pcorey pif didnt date girls met internet id never girlfriend brdarrell boy sets fire blockquote html ', ' joke told sucks ', ' lame plan best came with ', ' us number sucker know it ', ' expect people smart would willing wager none ever looked telescope read books astronomy really done either would doubts funny thing universe god made lot bigger complex than poor folks imagine well god less complex come anthropomorphic care one species living one planet orbiting one star one galaxy billion galaxies kind god creates billion billion stars worries sex lives naked apes would appear personal god ego trip simple mind d haas ']\n",
      "5 True Positives: [' want tell dont like see dont like decisions make ', ' want name better show me ', ' like crabs put one helluva fight always enjoyed trout line crabbing catching couple bushels hour two alas sold boat year to pay redskin season tickets crabs pout sandy go skins lobster dies meat starts disintegrating happens even restaurants lobsters cooked recently died meat still good long havent dead hours sometimes get lobster seems little meat it reason lobster alive kicking goes pot rhode islander ', ' geeky mother probably look suing twa yeah ridiculous blame twa ', ' want tell hate decision ', ' know jimbos getting little chatty merkins youve away joe jim know rules stop talking pubic wigs unless furthers diabolical scheme galactic domination pawn diabolical scheme know wit forgot evil laughter evil laughter axiomatic anyway distracted stroke cat feed sharks dream fiendishly stupid long winded ways offing meddling do gooders absent conventiently escape meddle anew instead shooting like rational megalomaniac evil genius rents due carribean island cant get staff days oi put charge anyway corporal pike say know know say oops big mouth thats spanking']\n",
      "5 True Negatives: [' dying know decision situation ', ' start actually started lasted around start didnt really kick gulf war launch wired magazine rise nirvana suppose one could argue twentieth century didnt really begin so ', ' actually see detail magnifications beyond high quality refractors long focal length high quality reflectors know conventional wisdom says no case included agree based views high quality inch refractor steady seeing conditions nights past several years seeing conditions observatory one night could see thin shadows walls craters moon several hours full moon nights experimented pushing mags past per inch aperture limit found image magnified could see detail moon mars jupiter on different nights sct optical system large secondary successfully seen detail mags much higher per inch aperture despite excellent seeing conditions yes image larger could see details conclude save money short focal length eyepieces learn see details lower mags oh tempted add tv radian would likely last eyepiece ever buy scope used sure nothing left see occassionally get lucky tell crazy naw crazy superb seeing conditions used old c observed mars zeiss markus ludes carries around pocket image beautiful course conditions like comes blue moon winter star party sweeping cobwebs edges mind get away see could find hope days lie ahead bring us back led listen whats said graham nash name david i nakamoto ', ' according ordered freedom theory consequence heat flowing spherical like body steady state gravimeter thermally isolated test mass obscured years gravity may related heat mass believe podkletnov effect demonstrates gravity heat related believe large spinning cold below k superconductor able cool things enough weight loss observed solar eclipse saxl observed weight gain torsion pendulum course high minded scientists gravimeters efforts thermally isolate everything hard time replicating podkletnovs saxls work question solar eclipse average temperature environment decrease dont first case temperature decrease causes weight loss second case temperature decrease causes weight gain ', ' let us know name written side blue card ', ' years ago wasnt market handful full time audio practioners today anyone creative musical talent one cost putting music distributable form this bottom line audio practice talking about become almost negligable allowing people buy business spend much time need order make living r a p tends kinda album centric wonder thats sometimes see things little differently work nothing making records recording tv shows dialog voice over work taking music recorded others molding new shapes sizes also live analog vs digital debate somewhat less significant circumstances though still irrelevant probably could record work could get case adat in garage aphobia agree room audio workers used be bear mind self producing album makers deal reliability compatibility deadline issues commercial music studio processes and certain extent objectives different show shot tuesday post thursday flutter vs jitter moves way list considerations g aint list top analog digital thing talking commercial music rooms one end bedroom studios other points lines get fuzzier often work studer board example mackie either digital facility except audio console would digital console better setting people use fairlights exactly cakewalk kind stuff scott dorsey describing either would better using analog thats segment referring to people working middle market protools et al set lorin ']\n"
     ]
    }
   ],
   "source": [
    "Offensive_Reddit_Kaggle_dict = test_for_train_dataset('Kumar','',datasets_dict,1,0.2,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4223,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict Founta__test_on_Kaggle_4.pickle', 'rb') as handle:\n",
    "    Founta_Kaggle_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>978</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>571</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0    1\n",
       "row_0          \n",
       "0      978   22\n",
       "1      571  429"
      ]
     },
     "execution_count": 4224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Founta_Kaggle_dict['best_confusion_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4225,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Waseem_test_on_Offensive_Reddit_new_RM_clean.pickle', 'rb') as handle:\n",
    "    Waseem_OR_dict_RM = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>810</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>580</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0    1\n",
       "row_0          \n",
       "0      810  190\n",
       "1      580  420"
      ]
     },
     "execution_count": 4226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Waseem_OR_dict_RM['best_confusion_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4227,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T10:21:54.862249Z",
     "iopub.status.busy": "2022-08-02T10:21:54.861689Z",
     "iopub.status.idle": "2022-08-02T10:21:54.872148Z",
     "shell.execute_reply": "2022-08-02T10:21:54.871225Z",
     "shell.execute_reply.started": "2022-08-02T10:21:54.862211Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset_with_Roastme(train_dataset_name,n):\n",
    "    RM_train = pd.read_csv('train_dataset_1_RoastMe_new.csv')\n",
    "    RM_train = RM_train.sample(frac=0.85)\n",
    "    RM_validation = pd.read_csv('validation_dataset_1_RoastMe_new.csv')\n",
    "    RM_validation = RM_validation.sample(frac=0.85)\n",
    "    for i in range(1,n+1):\n",
    "        train_dataset = pd.read_csv('train_dataset_'+ str(i)+'_'+train_dataset_name+'.csv')\n",
    "        train_dataset = pd.concat([train_dataset,RM_train])\n",
    "        train_dataset = train_dataset.sample(frac=1)\n",
    "        train_dataset = train_dataset.reset_index(drop=True)\n",
    "        train_dataset['id'] = range(1,len(train_dataset)+1)\n",
    "        train_dataset.to_csv('train_dataset_'+str(i)+'_'+train_dataset_name + '_RM.csv',index=False)\n",
    "        validation_dataset = pd.read_csv('validation_dataset_'+ str(i)+'_'+train_dataset_name+'.csv')\n",
    "        validation_dataset = pd.concat([validation_dataset,RM_validation])\n",
    "        validation_dataset = validation_dataset.sample(frac=1)\n",
    "        validation_dataset = validation_dataset.reset_index(drop=True)\n",
    "        validation_dataset['id'] = range(1,len(validation_dataset)+1)\n",
    "        validation_dataset.to_csv('validation_dataset_'+str(i)+'_'+train_dataset_name + '_RM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_with_second_dataset(train_dataset_name,second_dataset_name,n, fraction):\n",
    "    for i in range(1,n+1):\n",
    "        Second_train = pd.read_csv('train_dataset_' + str(i)+'_'+ second_dataset_name +  '.csv')\n",
    "        Second_train = Second_train.sample(frac=fraction)\n",
    "        Second_validation = pd.read_csv('validation_dataset_' + str(i)+'_'+ second_dataset_name +  '.csv')\n",
    "        Second_validation = Second_validation.sample(frac=fraction)\n",
    "        train_dataset = pd.read_csv('train_dataset_'+ str(i)+'_'+train_dataset_name+'.csv')\n",
    "        train_dataset = pd.concat([train_dataset,Second_train])\n",
    "        train_dataset = train_dataset.sample(frac=0.7)\n",
    "        train_dataset = train_dataset.reset_index(drop=True)\n",
    "        train_dataset['id'] = range(1,len(train_dataset)+1)\n",
    "        train_dataset.to_csv('train_dataset_'+str(i)+'_'+train_dataset_name +'_' +second_dataset_name + '.csv',index=False)\n",
    "        validation_dataset = pd.read_csv('validation_dataset_'+ str(i)+'_'+train_dataset_name+'.csv')\n",
    "        validation_dataset = pd.concat([validation_dataset,Second_validation])\n",
    "        validation_dataset = validation_dataset.sample(frac=0.7)\n",
    "        validation_dataset = validation_dataset.reset_index(drop=True)\n",
    "        validation_dataset['id'] = range(1,len(validation_dataset)+1)\n",
    "        validation_dataset.to_csv('validation_train_dataset_'+str(i)+'_'+train_dataset_name +'_' +second_dataset_name + '.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_dataset_with_second_dataset('Offensive_Reddit_new','Founta',3,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4230,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T10:21:54.874070Z",
     "iopub.status.busy": "2022-08-02T10:21:54.873463Z",
     "iopub.status.idle": "2022-08-02T10:21:54.887704Z",
     "shell.execute_reply": "2022-08-02T10:21:54.886730Z",
     "shell.execute_reply.started": "2022-08-02T10:21:54.873952Z"
    }
   },
   "outputs": [],
   "source": [
    "def devide_sataset_to_n_train_datasets(train_dataset_full,dataset_name,n):\n",
    "    #n = 5\n",
    "    train_dataset_full = train_dataset_full\n",
    "    train_class_len = math.floor(math.floor(len(train_dataset_full)/n)/2)\n",
    "    train_current = train_dataset_full\n",
    "    for i in range(n):\n",
    "        train_dataset = create_balanced_datasets(train_class_len,train_current[train_current['Label']==0],train_current[train_current['Label']==1])\n",
    "    #train_dataset = kaggle_dataset\n",
    "        train_current = pd.merge(train_current,train_dataset, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
    "        train_rate = 0.8\n",
    "        train_text, val_text, train_labels, val_labels = train_test_split(train_dataset['comment'], train_dataset['Label'], \n",
    "                                                                    #random_state=2018,\n",
    "                                                                    test_size= 1-train_rate,\n",
    "                                                                    stratify=train_dataset['Label'])\n",
    "        train_labels = train_labels.reset_index(drop=True)\n",
    "        train_text = train_text.reset_index(drop=True)\n",
    "        val_labels = val_labels.reset_index(drop=True)\n",
    "        val_text = val_text.reset_index(drop=True)\n",
    "        train_dataset_current = pd.DataFrame()\n",
    "        train_dataset_current['id'] = [i for i in range(1,len(train_text)+1)]\n",
    "        train_dataset_current['comment'] = train_text\n",
    "        train_dataset_current['Label'] = train_labels\n",
    "        validataion_dataset_current = pd.DataFrame()\n",
    "        validataion_dataset_current['id'] = [i for i in range(1,len(val_text)+1)]\n",
    "        validataion_dataset_current['comment'] = val_text\n",
    "        validataion_dataset_current['Label'] = val_labels\n",
    "        train_dataset_current.to_csv('train_dataset_' + str(i+1) +'_'+ dataset_name+'.csv',index=False, encoding=\"utf8\" )\n",
    "        validataion_dataset_current.to_csv('validation_dataset_' + str(i+1)+'_' + dataset_name+'.csv',index=False, encoding=\"utf8\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rm-project",
   "language": "python",
   "name": "rm-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
